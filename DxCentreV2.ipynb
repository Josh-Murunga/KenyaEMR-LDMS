{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11eee77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to database amani_medical\n",
      "Stored procedure create_etl_tables executed successfully.\n",
      "Stored procedure sp_first_time_setup executed successfully.\n",
      "Number of rows affected: 0\n",
      "Number of rows affected: 0\n",
      "Number of rows affected: 0\n",
      "Number of rows affected: 7\n",
      "SQL file api_scripts\\Current on ART.sql executed successfully.\n",
      "Number of rows affected: 0\n",
      "Number of rows affected: 0\n",
      "Number of rows affected: 0\n",
      "Number of rows affected: 0\n",
      "SQL file api_scripts\\HTS-OPD Tested.sql executed successfully.\n",
      "Pipeline completed for database amani_medical\n",
      "Connection to database amani_medical closed\n",
      "Connected to database openmrs\n",
      "Stored procedure create_etl_tables executed successfully.\n",
      "Stored procedure sp_first_time_setup executed successfully.\n",
      "Number of rows affected: 0\n",
      "Number of rows affected: 0\n",
      "Number of rows affected: 0\n",
      "Number of rows affected: 30\n",
      "SQL file api_scripts\\Current on ART.sql executed successfully.\n",
      "Number of rows affected: 0\n",
      "Number of rows affected: 0\n",
      "Number of rows affected: 0\n",
      "Number of rows affected: 17\n",
      "SQL file api_scripts\\HTS-OPD Tested.sql executed successfully.\n",
      "Pipeline completed for database openmrs\n",
      "Connection to database openmrs closed\n",
      "200\n",
      "{'httpStatus': 'OK', 'httpStatusCode': 200, 'status': 'OK', 'message': 'Import was successful.', 'response': {'responseType': 'ImportSummary', 'status': 'SUCCESS', 'importOptions': {'idSchemes': {}, 'dryRun': False, 'async': False, 'importStrategy': 'CREATE_AND_UPDATE', 'mergeMode': 'REPLACE', 'reportMode': 'FULL', 'skipExistingCheck': False, 'sharing': False, 'skipNotifications': False, 'skipAudit': False, 'datasetAllowsPeriods': False, 'strictPeriods': False, 'strictDataElements': False, 'strictCategoryOptionCombos': False, 'strictAttributeOptionCombos': False, 'strictOrganisationUnits': False, 'strictDataSetApproval': False, 'strictDataSetLocking': False, 'strictDataSetInputPeriods': False, 'requireCategoryOptionCombo': False, 'requireAttributeOptionCombo': False, 'skipPatternValidation': False, 'ignoreEmptyCollection': False, 'force': False, 'firstRowIsHeader': True, 'skipLastUpdated': False, 'mergeDataValues': False, 'skipCache': False}, 'description': 'Import process completed successfully', 'importCount': {'imported': 23, 'updated': 60, 'ignored': 0, 'deleted': 0}, 'conflicts': [], 'rejectedIndexes': [], 'dataSetComplete': 'false'}}\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Global properties\n",
    "startdate = '2024-06-01'\n",
    "enddate = '2024-06-30'\n",
    "period = '202406'\n",
    "\n",
    "# Folder containing SQL query files\n",
    "sql_folder = 'api_scripts'\n",
    "\n",
    "# MySQL database connection details\n",
    "db_user = 'root'\n",
    "db_password = 'test' # Set mysql password\n",
    "db_host = 'localhost'\n",
    "db_port = '3306'  # Default MySQL port is 3306\n",
    "db_name = 'ldwh'\n",
    "table_name = 'dataset_values'\n",
    "\n",
    "# Create a database connection engine\n",
    "engine = create_engine(f'mysql+pymysql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}')\n",
    "\n",
    "# Replace with your DHIS2 instance URL and credentials\n",
    "dhis2_url = 'LDMS URL'\n",
    "username = 'LDMS USER'\n",
    "password = 'LDMS PASSWORD'\n",
    "\n",
    "# Function to execute a stored procedure\n",
    "def execute_stored_procedure(cursor, proc_name):\n",
    "    try:\n",
    "        cursor.callproc(proc_name)\n",
    "        print(f\"Stored procedure {proc_name} executed successfully.\")\n",
    "    except Error as e:\n",
    "        print(f\"Error executing stored procedure {proc_name}: {e}\")\n",
    "\n",
    "# Function to execute SQL queries from files\n",
    "def execute_sql_file(cursor, file_path, params):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            query = file.read()\n",
    "        \n",
    "        # Replace placeholders with actual values\n",
    "        query = query.format(startDate=params['startdate'], endDate=params['enddate'], period=params['period'])\n",
    "        \n",
    "        for result in cursor.execute(query, multi=True):\n",
    "            if result.with_rows:\n",
    "                print(f\"Rows produced by query: {result.fetchall()}\")\n",
    "            else:\n",
    "                print(f\"Number of rows affected: {result.rowcount}\")\n",
    "\n",
    "        print(f\"SQL file {file_path} executed successfully.\")\n",
    "    except Error as e:\n",
    "        print(f\"Error executing SQL file {file_path}: {e}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "\n",
    "# Function to read database names from a CSV file\n",
    "def read_databases_from_csv(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df['database'].tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading database names from CSV: {e}\")\n",
    "        return []\n",
    "    \n",
    "# Function to post data to DHIS2\n",
    "def post_data_to_dhis2():\n",
    "    try:\n",
    "        # Read data from MySQL table into a DataFrame\n",
    "        query = f'SELECT * FROM {table_name}'\n",
    "        df = pd.read_sql(query, engine)\n",
    "\n",
    "        # Remove any rows with null values\n",
    "        df = df.dropna()\n",
    "\n",
    "        # Convert the DataFrame to the required JSON format\n",
    "        data_values = [\n",
    "            {\n",
    "                \"dataElement\": row['data_element'],\n",
    "                \"categoryOptionCombo\": row['category_option'],\n",
    "                \"orgUnit\": row['organization_unit'],\n",
    "                \"period\": row['period'],\n",
    "                \"value\": row['value']\n",
    "            }\n",
    "            for _, row in df.iterrows()\n",
    "        ]\n",
    "\n",
    "        data_value_set = {\n",
    "            \"dataValues\": data_values\n",
    "        }\n",
    "\n",
    "        json_data = json.dumps(data_value_set, indent=2)\n",
    "\n",
    "        # Headers for the request\n",
    "        headers = {\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "\n",
    "        # Make the POST request to the DHIS2 API\n",
    "        response = requests.post(dhis2_url, headers=headers, data=json_data, auth=(username, password))\n",
    "\n",
    "        # Print the response from the server\n",
    "        print(response.status_code)\n",
    "        print(response.json())\n",
    "    except Exception as e:\n",
    "        print(f\"Error posting data to DHIS2: {e}\")\n",
    "\n",
    "# Main function to run the pipeline\n",
    "def run_pipeline():\n",
    "    databases = read_databases_from_csv('databases.csv')\n",
    "    if not databases:\n",
    "        print(\"No databases to process.\")\n",
    "        return\n",
    "\n",
    "    sql_files = [os.path.join(sql_folder, f) for f in os.listdir(sql_folder) if f.endswith('.sql')]\n",
    "\n",
    "    for db in databases:\n",
    "        try:\n",
    "            # Connect to the database\n",
    "            connection = mysql.connector.connect(\n",
    "                host='localhost',\n",
    "                user='root',\n",
    "                password='test',\n",
    "                database=db\n",
    "            )\n",
    "            if connection.is_connected():\n",
    "                cursor = connection.cursor()\n",
    "                print(f\"Connected to database {db}\")\n",
    "\n",
    "                # Execute stored procedures\n",
    "                execute_stored_procedure(cursor, 'create_etl_tables')\n",
    "                execute_stored_procedure(cursor, 'sp_first_time_setup')\n",
    "\n",
    "                # Execute SQL queries from files\n",
    "                params = {'startdate': startdate, 'enddate': enddate, 'period': period}\n",
    "                for sql_file in sql_files:\n",
    "                    execute_sql_file(cursor, sql_file, params)\n",
    "\n",
    "                # Commit the transactions\n",
    "                connection.commit()\n",
    "                print(f\"Pipeline completed for database {db}\")\n",
    "\n",
    "        except Error as e:\n",
    "            print(f\"Error connecting to database {db}: {e}\")\n",
    "\n",
    "        finally:\n",
    "            if connection.is_connected():\n",
    "                cursor.close()\n",
    "                connection.close()\n",
    "                print(f\"Connection to database {db} closed\")\n",
    "    \n",
    "    # post data to dhis2\n",
    "    post_data_to_dhis2()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_pipeline()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117ca84a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0616edee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d982475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
